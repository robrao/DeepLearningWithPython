{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLP-imdb_reviews.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3dHPR2UFps3",
        "colab_type": "text"
      },
      "source": [
        "Use the IMDB dataset packaged with Keras. It has already been labelled, where each word has been turned into an integer. We need to modify `np.load` to set `allow_pickle=True` otherwise we can't load the data. When loading the imdb data we use `num_words=10000` so that we only get the 10k most frequently used words. Although we lose information to train the neural net it allows our data to remain tractable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWtqNcNcGJmu",
        "colab_type": "code",
        "outputId": "092f6dc9-36d2-41ef-f3d3-ac3dc88a51d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# Set allow_pickle to True since new version defaults to False\n",
        "old = np.load\n",
        "np.load = lambda *a,**k: old(*a, allow_pickle=True, **k)\n",
        "\n",
        "# Keep only 10k most commonly used words, rest discarded, to keep vectors of tractable size\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "#print ('Training Data: ', len(train_data), 'Testing Data: ', len(test_data))\n",
        "#print ('Training Data Example: ', train_data[3], ' Label: ', test_labels[3])\n",
        "\n",
        "np.load = old\n",
        "del(old)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uHBV8cae2Sl",
        "colab_type": "text"
      },
      "source": [
        "Creating a reverse dictionary look to see what the vectors orginally looked like to get a sense of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbGkaddAfDYU",
        "colab_type": "code",
        "outputId": "2f19b47b-0268-4be1-bb44-beacf882e3e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Decoding encoded vector\n",
        "word_index = imdb.get_word_index();\n",
        "reverse_word_index = dict(\n",
        "  [(value, key) for (key, value) in word_index.items()]\n",
        ")\n",
        "# Shift the index by 3 because the first 3 places are reserved for special characters\n",
        "decoded_review = ' '.join(\n",
        "  [reverse_word_index.get(i-3, '?') for i in train_data[0]]\n",
        ")\n",
        "\n",
        "#print ('Decoded review of above: ', decoded_review)\n",
        "#print ('Label: ', train_labels[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2FYht-xfT7Y",
        "colab_type": "text"
      },
      "source": [
        " Along the dimension axis all integers (words) contained in the sequence will become 1s. This means we lost all information contained in the ordering of the words, which is obvisously very important, but the NN will still learn having certain words in a sentence is good and other are bad. This allows our first layer of the NN to be a dense layer capable of handling floating point vector data.\n",
        "\n",
        "### eg:\n",
        " `(3, [2, 34, 9934, 62, 88])`indicates that row 2 in the matrix will have indices 2, 34, 9934, 62, 88 as 1.\n",
        " and everything else in the dimension axis will be 0.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_1815uVgEk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n",
        "#print ('Train example: ', x_train[0])\n",
        "#print ('Train label: ', y_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO0-falQLCtJ",
        "colab_type": "text"
      },
      "source": [
        "Input data is vectors, and the labels are scalars, a type of network that performs well on such a problem is a simple stack of fully connected layers with RELU activations. Can be thought of as `output = relu(dot(W, input) + b)`. `W` has the dimensionality `(input_width_and_height, given_depth_dimension)`, since its a fully connected layer (DENSE) it's width and height match the input data. Where as with convolutional layers the width or height (can't remember which) must match the input data where as the other can be decided.\n",
        "\n",
        "\"You can intuitively understand the dimensionality of your representation space as 'how much freedom you're allowing the network to have when learning internal representations'\"\n",
        "\n",
        "If you dimensionality is to too high, too much freedom, and you don't have enough data the neural net will end up memorizing the training data and will not be able to generalize.\n",
        "\n",
        "* The key architectural decision of dense layers is to decide how many layers to use and their dimensionality.\n",
        "* Using relu in intermediate layers. RELU is preferred in intermediate layers because in it prevents backprop degradation or explosion, but this would not be a concern in such a small NN.\n",
        "* The final layer will use a sigmoid activation to output a probability of a review being positive. Sigmoid activations squash values into the 0-1 interval.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VnyaiJrNunn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) # one hot encoded vector of 10k\n",
        "model.add(layers.Dense(16, activation='relu')) #infers input dimension from previous layer\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5Wk9Vc9Pk1y",
        "colab_type": "text"
      },
      "source": [
        " Activations allow neural nets to learn non-linear transformations, otherwise would just be layers of linear transformations which could only represent linear transformations. Obvisously deep linear transformations would provide not benefit, eg. the hypothesis space of such a layer would be the set of all possible linear transformtions of the input data into 16 dimensional space. (Note: expound on this)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmN2QvlmQEiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import losses\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "#model.compile(optimizer='rmsprop',\n",
        "#             loss='binary_crossentropy',\n",
        "#             metrics=['accuracy'])\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "             loss=losses.binary_crossentropy,\n",
        "             metrics=[metrics.binary_accuracy])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDZzv81lQWPc",
        "colab_type": "text"
      },
      "source": [
        "Could use mean squared error as the loss, but since dealing with probability as the output it best to use cross entropy which measures the difference between probability distributions. \n",
        "The optimizer describes the type of backpropagation method you want use. The different types of optimizers have different approachs such as using momentum, learning rate, learning rate decay, etc. [This](http://ruder.io/optimizing-gradient-descent/) provides a great overview of different optimizers. "
      ]
    }
  ]
}